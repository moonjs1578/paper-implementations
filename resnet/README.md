# ResNet Implementation

PyTorch implementation of **Deep Residual Learning for Image Recognition** (CVPR 2015)

## ğŸ“„ Paper Information

- **Title**: Deep Residual Learning for Image Recognition
- **Authors**: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun (Microsoft Research)
- **Conference**: CVPR 2015 (Best Paper Award ğŸ†)
- **Paper**: [arXiv:1512.03385](https://arxiv.org/abs/1512.03385)

## ğŸ¯ Key Contributions

### 1. Residual Learning
- **ë¬¸ì œ**: ë„¤íŠ¸ì›Œí¬ê°€ ê¹Šì–´ì§ˆìˆ˜ë¡ degradation ë°œìƒ (ì„±ëŠ¥ ì €í•˜)
- **í•´ê²°**: Skip connection (shortcut connection)ì„ í†µí•œ residual learning
- **í•µì‹¬ ì•„ì´ë””ì–´**: `F(x) + x` - ì”ì°¨(residual)ë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì´ ë” ì‰¬ì›€

### 2. Network Architecture

#### BasicBlock (ResNet-18, 34)
```
x â†’ [3x3 conv] â†’ [3x3 conv] â†’ + â†’ output
|                                |
+--------- skip connection ------+
```

#### Bottleneck (ResNet-50, 101, 152)
```
x â†’ [1x1 conv] â†’ [3x3 conv] â†’ [1x1 conv] â†’ + â†’ output
|  (reduce)       (process)      (expand)    |
+--------------- skip connection -------------+
```

### 3. Why ResNet Works?

1. **Gradient Flow**: Skip connectionì´ gradientë¥¼ ì§ì ‘ ì „íŒŒ
2. **Identity Mapping**: ìµœì•…ì˜ ê²½ìš°ì—ë„ ì…ë ¥ì„ ê·¸ëŒ€ë¡œ ì „ë‹¬ ê°€ëŠ¥
3. **Ensemble Effect**: ë‹¤ì–‘í•œ ê¹Šì´ì˜ ë„¤íŠ¸ì›Œí¬ë¥¼ ì•™ìƒë¸”í•˜ëŠ” íš¨ê³¼

## ğŸ—ï¸ Model Variants

| Model | Blocks | Layers | Parameters | Top-1 Error (ImageNet) |
|-------|--------|--------|------------|----------------------|
| ResNet-18 | BasicBlock | [2,2,2,2] | 11.7M | 30.24% |
| ResNet-34 | BasicBlock | [3,4,6,3] | 21.8M | 26.70% |
| ResNet-50 | Bottleneck | [3,4,6,3] | 25.6M | 24.01% |
| ResNet-101 | Bottleneck | [3,4,23,3] | 44.5M | 22.44% |
| ResNet-152 | Bottleneck | [3,8,36,3] | 60.2M | 21.69% |

## ğŸ“ File Structure

```
resnet/
â”œâ”€â”€ __init__.py          # Package initialization
â”œâ”€â”€ blocks.py            # BasicBlock & Bottleneck ì •ì˜
â”œâ”€â”€ model.py             # ResNet ëª¨ë¸ ì •ì˜
â”œâ”€â”€ train.py             # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â””â”€â”€ README.md            # This file
```

## ğŸš€ Usage

### 1. Import Model
```python
from resnet import resnet18, resnet34, resnet50

# Create model
model = resnet18(num_classes=10)  # for CIFAR-10
model = resnet50(num_classes=1000)  # for ImageNet
```

### 2. Forward Pass
```python
import torch

x = torch.randn(1, 3, 224, 224)  # [batch, channels, height, width]
output = model(x)  # [1, num_classes]
```

### 3. Training (CIFAR-10)
```bash
cd resnet
python train.py
```

## ğŸ’¡ Learning Points (TODO êµ¬í˜„í•˜ë©´ì„œ ë°°ìš¸ ë‚´ìš©)

### 1. Residual Connection
- `F(x) + x`ì˜ ì˜ë¯¸ì™€ êµ¬í˜„
- Skip connectionì´ gradient flowì— ë¯¸ì¹˜ëŠ” ì˜í–¥
- Identity mappingì˜ ì¤‘ìš”ì„±

### 2. Bottleneck Design
- 1x1 convolutionì˜ ì—­í•  (dimensionality reduction/expansion)
- ê³„ì‚°ëŸ‰ ì ˆê° íš¨ê³¼
- ê¹Šì€ ë„¤íŠ¸ì›Œí¬ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ë§Œë“œëŠ” ë°©ë²•

### 3. Downsampling
- Strideë¥¼ ì´ìš©í•œ spatial dimension ì¶•ì†Œ
- Skip connectionì˜ í¬ê¸°/ì±„ë„ ë§ì¶”ê¸°
- 1x1 convolutionì„ ì´ìš©í•œ projection

### 4. Training Techniques
- Kaiming He initialization
- Batch Normalizationì˜ ìœ„ì¹˜
- Learning rate scheduling
- Data augmentation

## ğŸ”¬ Implementation Details

### Network Architecture
```
Input (3Ã—224Ã—224)
    â†“
Conv1: 7Ã—7, 64, stride=2
BatchNorm + ReLU
MaxPool: 3Ã—3, stride=2
    â†“
Layer1: [64, 64, 64, ...] (stride=1)
Layer2: [128, 128, 128, ...] (stride=2)  â† downsampling
Layer3: [256, 256, 256, ...] (stride=2)  â† downsampling
Layer4: [512, 512, 512, ...] (stride=2)  â† downsampling
    â†“
Global Average Pooling
Fully Connected (â†’ num_classes)
```

### Training Hyperparameters (ImageNet)
- **Optimizer**: SGD with momentum (0.9)
- **Weight decay**: 0.0001
- **Batch size**: 256
- **Learning rate**: 0.1, divided by 10 at 30k, 60k iterations
- **Epochs**: 90
- **Data augmentation**: Random crop, horizontal flip

### CIFAR-10 Adaptation
- ë” ì‘ì€ ì…ë ¥ í¬ê¸° (32Ã—32)
- ì²« ë²ˆì§¸ conv: 3Ã—3, stride=1 (7Ã—7 ëŒ€ì‹ )
- MaxPool ì œê±°
- ë” ê¸´ í•™ìŠµ (200 epochs)

## ğŸ“Š Expected Results (CIFAR-10)

| Model | Parameters | Test Accuracy |
|-------|------------|---------------|
| ResNet-18 | ~11M | ~95% |
| ResNet-34 | ~21M | ~95.5% |
| ResNet-50 | ~23M | ~96% |

## ğŸ“ Key Takeaways

1. **Depth Matters**: ì ì ˆí•œ êµ¬ì¡°(residual connection)ë§Œ ìˆë‹¤ë©´ ê¹Šì€ ë„¤íŠ¸ì›Œí¬ê°€ ë” ì¢‹ìŒ
2. **Skip Connections**: Gradient vanishing ë¬¸ì œ í•´ê²°ì˜ í•µì‹¬
3. **Bottleneck Design**: ê³„ì‚° íš¨ìœ¨ì„±ê³¼ í‘œí˜„ë ¥ì˜ ê· í˜•
4. **Simplicity**: ë³µì¡í•œ ê¸°ë²• ì—†ì´ë„ ê°•ë ¥í•œ ì„±ëŠ¥

## ğŸ”— References

- [Original Paper](https://arxiv.org/abs/1512.03385)
- [PyTorch Official Implementation](https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py)
- [Author's Slides](https://icml.cc/2016/tutorials/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf)

## ğŸ“ TODO Checklist

í•™ìŠµì„ ìœ„í•´ ë‹¤ìŒ ë¶€ë¶„ë“¤ì„ ì§ì ‘ êµ¬í˜„í•´ë³´ì„¸ìš”:

### blocks.py
- [ ] BasicBlockì˜ convolution layers
- [ ] BasicBlockì˜ forward pass (residual connection)
- [ ] Bottleneckì˜ 1x1-3x3-1x1 êµ¬ì¡°
- [ ] Bottleneckì˜ forward pass

### model.py
- [ ] ResNetì˜ initial layers (conv1, maxpool)
- [ ] `_make_layer` ë©”ì„œë“œì˜ downsample ë¡œì§
- [ ] ResNetì˜ forward pass
- [ ] Weight initialization

### train.py
- [ ] Data augmentation transforms
- [ ] Training loop (forward, backward, optimizer step)
- [ ] Validation loop
- [ ] Learning rate scheduler

---

â­ **Tip**: êµ¬í˜„í•˜ë©´ì„œ ë§‰íˆë©´ ë…¼ë¬¸ì˜ Figure 3, Table 1ì„ ì°¸ê³ í•˜ì„¸ìš”!
