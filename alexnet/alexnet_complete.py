# ============================================================
# AlexNet êµ¬í˜„ í…œí”Œë¦¿
# ë…¼ë¬¸: ImageNet Classification with Deep Convolutional
#        Neural Networks (Krizhevsky et al., 2012)
#
# ğŸ“ ê·œì¹™:
#   - TODO: ì§ì ‘ êµ¬í˜„í•´ì•¼ í•  ë¶€ë¶„
#   - íŒíŠ¸: ë§‰íˆë©´ ì°¸ê³ í•˜ì„¸ìš”
#   - ì°¸ê³ : ìš°ë¦¬ê°€ ê³µë¶€í•œ ë‚´ìš© ë³µê·€ìš©
# ============================================================

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset


# ============================================================
# 1. AlexNet ëª¨ë¸ ì •ì˜
# ============================================================
class AlexNet(nn.Module):
    def __init__(self, num_classes=10):
        super(AlexNet, self).__init__()
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # TODO 1: self.features ì •ì˜ (Conv Layers 5ê°œ)
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # ì°¸ê³ : ì´ êµ¬ì¡°ë¥¼ ë”°ë¼ë¼
        #
        # Conv1 â†’ ReLU â†’ LRN â†’ Pool
        # Conv2 â†’ ReLU â†’ LRN â†’ Pool
        # Conv3 â†’ ReLU
        # Conv4 â†’ ReLU
        # Conv5 â†’ ReLU â†’ Pool
        #
        # íŒíŠ¸ (ê° Conv ì„¤ì •):
        # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        # â”‚  Layer  â”‚in_ch   â”‚  out_ch  â”‚  k     â”‚ stride  â”‚
        # â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        # â”‚  Conv1  â”‚  3     â”‚   96     â”‚  11    â”‚    4    â”‚
        # â”‚  Conv2  â”‚  96    â”‚  256     â”‚   5    â”‚    1    â”‚
        # â”‚  Conv3  â”‚ 256    â”‚  384     â”‚   3    â”‚    1    â”‚
        # â”‚  Conv4  â”‚ 384    â”‚  384     â”‚   3    â”‚    1    â”‚
        # â”‚  Conv5  â”‚ 384    â”‚  256     â”‚   3    â”‚    1    â”‚
        # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        #
        # íŒíŠ¸ (padding):
        #   Conv1: padding=0
        #   Conv2~5: padding = kernel_size // 2  (ì¶œë ¥ í¬ê¸° ìœ ì§€)
        #
        # íŒíŠ¸ (LRN):
        #   nn.LocalResponseNorm(size=5, alpha=1e-4, beta=0.75, k=2)
        #   â†’ Conv1, Conv2 í›„ì—ë§Œ!
        #
        # íŒíŠ¸ (Pool):
        #   nn.MaxPool2d(kernel_size=3, stride=2)
        #   â†’ Conv1, Conv2, Conv5 í›„ì—ë§Œ!

        self.features = nn.Sequential(
            # === Conv Layer 1 ===
            # ì…ë ¥: 224Ã—224Ã—3 â†’ ì¶œë ¥: 54Ã—54Ã—96
            nn.Conv2d(3, 96, 11, 4),
            nn.ReLU(),
            nn.LocalResponseNorm(size=5, alpha=1e-4, beta=0.75, k=2),
            nn.MaxPool2d(kernel_size=3, stride=2),
    
            # === Conv Layer 2 ===
            # ì¶œë ¥: 26Ã—26Ã—256
            nn.Conv2d(96, 256, 5, 1, 2),
            nn.ReLU(),
            nn.LocalResponseNorm(size=5, alpha=1e-4, beta=0.75, k=2),
            nn.MaxPool2d(kernel_size=3, stride=2),

            # === Conv Layer 3 ===
            # ì¶œë ¥: 12Ã—12Ã—384
            nn.Conv2d(256, 384, 3, 1, 1),
            nn.ReLU(),

            # === Conv Layer 4 ===
            # ì¶œë ¥: 12Ã—12Ã—384
            nn.Conv2d(384, 384, 3, 1, 1),
            nn.ReLU(),

            # === Conv Layer 5 ===
            # ì¶œë ¥: 5Ã—5Ã—256
            nn.Conv2d(384, 256, 3, 1, 1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=3, stride=2)
        )

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # TODO 2: self.avgpool ì •ì˜
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # íŒíŠ¸: nn.AdaptiveAvgPool2d ì‚¬ìš©
        # ì¶œë ¥ í¬ê¸°ë¥¼ 6Ã—6ìœ¼ë¡œ ê³ ì •í•˜ë¼
        # â†’ FC Layer ì…ë ¥ í¬ê¸°ê°€ í•­ìƒ ë™ì¼í•˜ê²Œ ë¨
        self.avgpool = nn.AdaptiveAvgPool2d(6)

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # TODO 3: self.classifier ì •ì˜ (FC Layers 3ê°œ)
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # ì°¸ê³ : ì´ êµ¬ì¡°ë¥¼ ë”°ë¼ë¼
        #
        # Dropout â†’ FC1 â†’ ReLU
        # Dropout â†’ FC2 â†’ ReLU
        # FC3
        #
        # íŒíŠ¸:
        #   FC1 ì…ë ¥: avgpool ì¶œë ¥ì„ Flattení•œ í›„ í¬ê¸°
        #          = 256 Ã— 6 Ã— 6 = ?
        #   FC1 ì¶œë ¥: 4096
        #   FC2 ì¶œë ¥: 4096
        #   FC3 ì¶œë ¥: num_classes
        #   Dropout: p=0.5 (FC1, FC2 ì•ì—ë§Œ!)

        self.classifier = nn.Sequential(
            # === FC Layer 1 ===
            nn.Dropout(p=0.5),
            nn.Linear(256*6*6, 4096),
            nn.ReLU(),
            # === FC Layer 2 ===
            nn.Dropout(p=0.5),
            nn.Linear(4096, 4096),
            nn.ReLU(),
            # === FC Layer 3 (ì¶œë ¥ì¸µ) ===
            nn.Linear(4096, num_classes)
            # â€» SoftmaxëŠ” ì•„ë˜ Lossì— í¬í•¨ â†’ ì—¬ê¸°ì„œëŠ” ì•ˆ ì”€!
        )

    def forward(self, x):
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # TODO 4: Forward Pass êµ¬í˜„
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # íŒíŠ¸: ìˆœì„œëŒ€ë¡œ í†µê³¼ì‹œí‚¤ë¼
        #   1. self.features(x)
        #   2. self.avgpool(x)
        #   3. torch.flatten(x, 1)   â† 1ë²ˆì§¸ dimë¶€í„° flatten
        #   4. self.classifier(x)
        #   5. return x
        x = self.features(x)
        x = self.avgpool(x)
        x = torch.flatten(x,1)
        x = self.classifier(x)
        return x


# ============================================================
# 2. Training Loop
# ============================================================
def train(model, dataloader, criterion, optimizer):
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # TODO 5: í•™ìŠµ ë£¨í”„ êµ¬í˜„
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # íŒíŠ¸: ì´ ìˆœì„œë¡œ êµ¬í˜„í•˜ë¼
    #
    #   1. model.train()           â†’ í•™ìŠµ ëª¨ë“œ ON (Dropout ì‘ë™!)
    #   2. for data, target in dataloader:
    #   3.     optimizer.zero_grad()   â†’ gradient ì´ˆê¸°í™”
    #   4.     output = model(data)    â†’ ìˆœë°©í–¥
    #   5.     loss = criterion(output, target)  â†’ loss ê³„ì‚°
    #   6.     loss.backward()         â†’ ì—­ì „íŒŒ
    #   7.     optimizer.step()        â†’ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
    #   8.     loss ì¶œë ¥
    model.train()
    for data, target in dataloader:
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        print(f"Loss: {loss.item():.4f}")


def test(model, dataloader, criterion):
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # TODO 6: í…ŒìŠ¤íŠ¸ ë£¨í”„ êµ¬í˜„
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # íŒíŠ¸: trainê³¼ ë‹¤ë¥¸ ì  2ê°€ì§€!
    #   1. model.eval()            â†’ í…ŒìŠ¤íŠ¸ ëª¨ë“œ (Dropout ë„ê¸°!)
    #   2. with torch.no_grad():   â†’ gradient ì•ˆ ê³„ì‚° (ì†ë„â†‘ ë©”ëª¨ë¦¬â†“)
    #
    # ë‚˜ë¨¸ì§€ëŠ” trainê³¼ ë™ì¼í•˜ê²Œ:
    #   output â†’ loss ê³„ì‚° â†’ ì¶œë ¥
    model.eval()
    with torch.no_grad():
        for data, target in dataloader:
            output = model(data)
            loss = criterion(output, target)
            print(f"Test Loss: {loss.item():.4f}")
# ============================================================
# 3. Main
# ============================================================
if __name__ == '__main__':

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # TODO 7: ì•„ë˜ ìˆœì„œëŒ€ë¡œ êµ¬í˜„í•˜ë¼
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    # 1. ëª¨ë¸ ìƒì„±
    #    íŒíŠ¸: model = AlexNet(num_classes=10)
    #    ì°¸ê³ : ì›ë³¸ ë…¼ë¬¸ì€ 1000ê°œ, í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 10ê°œ
    model = AlexNet(num_classes=10)

    # 2. ë”ë¯¸ ë°ì´í„° ìƒì„±
    #    íŒíŠ¸:
    #      train_data   = torch.randn(32, 3, 224, 224)
    #      train_labels = torch.randint(0, 10, (32,))
    #      test_data    = torch.randn(8, 3, 224, 224)
    #      test_labels  = torch.randint(0, 10, (8,))
    #
    #    DataLoaderë¡œ ê°ì‹¸ê¸°:
    #      train_loader = DataLoader(
    #          TensorDataset(train_data, train_labels),
    #          batch_size=8, shuffle=True
    #      )
    #      test_loader  = DataLoader(...)
    train_data = torch.randn(32, 3, 224, 224)
    train_labels = torch.randint(0, 10, (32,))
    test_data    = torch.randn(8, 3, 224, 224)
    test_labels  = torch.randint(0, 10, (8,))

    train_loader = DataLoader(
        TensorDataset(train_data, train_labels),
        batch_size = 8, shuffle= True
    )
    test_loader = DataLoader(
        TensorDataset(test_data, test_labels),
        batch_size=8, shuffle=True
    )

    # 3. Loss ì •ì˜
    #    íŒíŠ¸: nn.CrossEntropyLoss()
    #    ì°¸ê³ : Softmax + NLLLoss í¬í•¨!
    criterion = nn.CrossEntropyLoss()

    # 4. Optimizer ì •ì˜
    #    íŒíŠ¸: optim.SGD(
    #        model.parameters(),
    #        lr=0.01,
    #        momentum=0.9,
    #        weight_decay=5e-4
    #    )
    optimizer = optim.SGD(
        model.parameters(),
        lr=0.01,
        momentum=0.9,
        weight_decay=5e-4
    )

    # 5. í•™ìŠµ ë£¨í”„
    #    íŒíŠ¸:
    #    EPOCHS = 3
    #    for epoch in range(EPOCHS):
    #        train(...)
    #        test(...)
    EPOCHS = 3
    for epoch in range(EPOCHS):
        print(f"\nEpoch {epoch+1}/{EPOCHS}")
        train(model, train_loader, criterion, optimizer)
        test(model, test_loader, criterion)